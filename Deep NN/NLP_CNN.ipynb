{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras RNN\n",
    "This notebook follows the reference below.\n",
    "\n",
    "http://ethen8181.github.io/machine-learning/keras/rnn_language_model_basic_keras.html#Keras-RNN-(Recurrent-Neural-Network)---Language-Model\n",
    "\n",
    "The goal of this project is given n word, predict n+1th word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plaidml.keras #GPU packages\n",
    "plaidml.keras.install_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/uozdemir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/uozdemir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ulas 2020-07-03 07:38:52 \n",
      "\n",
      "CPython 3.7.6\n",
      "IPython 7.12.0\n",
      "\n",
      "keras 2.2.4\n",
      "numpy 1.18.1\n",
      "matplotlib 3.1.3\n",
      "tensorflow 1.13.1\n",
      "nltk 3.4.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "path = os.getcwd() # Get the current working directory\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "# 1. magic so that the notebook will reload external python modules\n",
    "# 2. magic for inline plot\n",
    "# 3. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "# 4. magic to print version\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext watermark\n",
    "%watermark -a 'Ulas' -d -t -v -p keras,numpy,matplotlib,tensorflow,nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of word-level language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n",
      "example text: PREFACE\n",
      "\n",
      "\n",
      "SUPPOSING that Truth is a woman--what then? Is there not ground\n",
      "for suspecting that all philosophers, in so far as they have been\n",
      "dogmatists\n"
     ]
    }
   ],
   "source": [
    "pathFile = get_file('nietzsche.txt', origin = 'https://s3.amazonaws.com/text-datasets/nietzsche.txt') # Download the text file\n",
    "\n",
    "with open(pathFile, encoding = 'utf-8') as f:\n",
    "    raw_text = f.read() # Save the text file raw_text variable\n",
    "\n",
    "print('corpus length:', len(raw_text)) # Print the char length of the corpus\n",
    "print('example text:', raw_text[:150]) # Print the first 150 char of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to remove punctuation marks, and '--' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled original text:  ['PREFACE', 'SUPPOSING', 'that', 'Truth', 'is', 'a', 'woman', 'what', 'then?', 'Is']\n",
      "sampled cleaned text:  ['preface', 'supposing', 'that', 'truth', 'is', 'a', 'woman', 'what', 'then', 'is']\n"
     ]
    }
   ],
   "source": [
    "# ideally, we would save the cleaned text, to prevent\n",
    "# doing this step every single time\n",
    "porterStemmer = PorterStemmer() # Initialize stemmer\n",
    "tokens = raw_text.replace('--', ' ').split()\n",
    "cleanedTokens = []\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for word in tokens:\n",
    "    word = word.translate(table)\n",
    "#     word = porterStemmer.stem(word)\n",
    "    if word.isalpha():\n",
    "        cleanedTokens.append(word.lower())\n",
    "        \n",
    "print('sampled original text: ', tokens[:10])\n",
    "print('sampled cleaned text: ', cleanedTokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of storing each word separetly, it is a good practice to assign each word an index. I believe this is because hashtable lookups are faster than matrix lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words:  5090 , Filtered Words:  5097\n"
     ]
    }
   ],
   "source": [
    "min_count = 2 # The word should appear more than once. This is necessary to get rid of noise.\n",
    "unknown_token = '<unk>' # If a word is not in the vocabulary, it will be classified as <unk>\n",
    "word2index = {unknown_token: 0}\n",
    "index2word = [unknown_token]\n",
    "\n",
    "\n",
    "stopWords = set(stopwords.words('english')) # Generate a list of English stopwords.\n",
    "filteredWords = 0\n",
    "counter = Counter(cleanedTokens) # this dictionary will store how many times each word showed up in the corpus.\n",
    "for word, count in counter.items():\n",
    "    if count >= min_count:# and word not in stopWords: # Check if the word shows up more than twice\n",
    "        index2word.append(word) # append the word to index2word vector\n",
    "        word2index[word] = len(word2index) #the location of the word in index2word is the index on the dict.\n",
    "    else:\n",
    "        filteredWords += 1 # If the word only showed up once, \n",
    "vocabSize = len(word2index)\n",
    "\n",
    "print('Number of Words: ', vocabSize,', Filtered Words: ', filteredWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence dimension:  (33342, 40)\n",
      "target dimension:  (33342, 5090)\n",
      "example sequence:\n",
      " [ 1  2  3  4  5  6  7  8  9  5 10 11 12 13  0  3 14 15 16 17 18 19 20 21\n",
      " 22 23 21 24 25 26 27  3 28 29 30 31 32  0 33 34]\n"
     ]
    }
   ],
   "source": [
    "step = 3\n",
    "maxlen = 40\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# This part will create \"sentences\" with fixed lengths of 40 words. Since we are trying to predict (n+1)th word\n",
    "# labels are generated using the next word after each 40 words.\n",
    "# Step parameter controls the overlapping between sentences. \n",
    "\n",
    "for i in range(0, len(cleanedTokens) - maxlen, step):\n",
    "    sentence = cleanedTokens[i:i + maxlen] # A sentence is a group of 40 words starting from index i.\n",
    "    next_word = cleanedTokens[i + maxlen] # Labels are 41th word of each group\n",
    "    X.append([word2index.get(word, 0) for word in sentence]) # Find the index of each word in a sentence, then append it to feature vector\n",
    "    y.append(word2index.get(next_word, 0)) # Find the index of the label in the dictionary append it to vector y.\n",
    "\n",
    "X = np.array(X) # Convert list into a numpy array\n",
    "Y = to_categorical(y, vocabSize) # Keras expects label vector to be one hot encoded.\n",
    "\n",
    "print('sequence dimension: ', X.shape) # There are 33342 sentences. Each sentence contains 40 words.\n",
    "print('target dimension: ', Y.shape) # Note that 5090 is the vocabulary size.\n",
    "print('example sequence:\\n', X[0]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"metal_amd_radeon_pro_5300m.0\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 50)            254500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               314368    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5090)              1308130   \n",
      "=================================================================\n",
      "Total params: 1,876,998\n",
      "Trainable params: 1,876,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embeddingSize = 50 # Embedding vector size\n",
    "lstmSize = 256 # LSTM layer size\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embeddingSize, input_length = maxlen)) # Embedding layer will generate vectors\n",
    "model.add(LSTM(lstmSize))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(vocabSize, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam') # Adam is the safest bet.\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts seconds into human-readable format\n",
    "def elapsed(sec):\n",
    "    if sec < 60:\n",
    "        return str(sec) + ' seconds'\n",
    "    elif sec < (60 * 60):\n",
    "        return str(sec / 60) + ' minutes'\n",
    "    else:\n",
    "        return str(sec / (60 * 60)) + ' hours'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model checkpoint address:  lstm_weights1.hdf5\n",
      "Train on 26673 samples, validate on 6669 samples\n",
      "Epoch 1/40\n",
      "26673/26673 [==============================] - 125s 5ms/step - loss: 6.3486 - val_loss: 6.1974\n",
      "Epoch 2/40\n",
      "26673/26673 [==============================] - 119s 4ms/step - loss: 5.8109 - val_loss: 6.1621\n",
      "Epoch 3/40\n",
      "26656/26673 [============================>.] - ETA: 0s - loss: 5.5723"
     ]
    }
   ],
   "source": [
    "def build_model(model, address = None):\n",
    "    if address is not None or not os.path.isfile(address): # Check is a checkpoint exists. If none exists fit the model\n",
    "        stop = EarlyStopping(monitor = 'val_loss', min_delta = 0, \n",
    "                             patience = 5, verbose = 1, mode = 'auto') # use validation loss as a monitoring tool for early stopping.\n",
    "        save = ModelCheckpoint(address, monitor = 'val_loss', \n",
    "                               verbose = 0, save_best_only = True) # Save the model\n",
    "        callbacks = [stop, save]\n",
    "\n",
    "        start = time()\n",
    "        history = model.fit(X, Y, batch_size = batch_size, \n",
    "                            epochs = epochs, verbose = 1,\n",
    "                            validation_split = validation_split,\n",
    "                            callbacks = callbacks)\n",
    "        elapse = time() - start\n",
    "        print('elapsed time: ', elapsed(elapse))\n",
    "        model_info = {'history': history, 'elapse': elapse, 'model': model}\n",
    "    else: # If a checkpoints exists load it.\n",
    "        model = load_model(address)\n",
    "        model_info = {'model': model}\n",
    "\n",
    "    return model_info\n",
    "  \n",
    "\n",
    "epochs = 40\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "address1 = 'lstm_weights1.hdf5'\n",
    "print('model checkpoint address: ', address1)\n",
    "model_info = build_model(model, address1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual words: they paid to been unseemly <unk> certainly never to and \n",
      "Predicted words: the down and to <unk> the is been to and \n"
     ]
    }
   ],
   "source": [
    "def check_prediction(model, num_predict):\n",
    "    true_print_out = 'Actual words: '\n",
    "    pred_print_out = 'Predicted words: '\n",
    "    for i in range(num_predict):\n",
    "        x = X[i]\n",
    "        prediction = model.predict(x[np.newaxis, :], verbose = 0)\n",
    "        index = np.argmax(prediction)\n",
    "        true_print_out += index2word[y[i]] + ' '\n",
    "        pred_print_out += index2word[index] + ' '\n",
    "\n",
    "    print(true_print_out)\n",
    "    print(pred_print_out)\n",
    "\n",
    "\n",
    "num_predict = 10\n",
    "model = model_info['model']\n",
    "check_prediction(model, num_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords does not seem to be a good idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
